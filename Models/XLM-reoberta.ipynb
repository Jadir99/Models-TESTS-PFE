{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b22bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded : Babelscape/wikineural-multilingual-ner\n",
      "Labels       : {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================\n",
    "# Model: Babelscape/wikineural-multilingual-ner\n",
    "#   - Based on XLM-RoBERTa-large\n",
    "#   - Trained on WikiNEuRal (9 languages incl. FR + EN)\n",
    "#   - Significantly better cross-lingual NER than CoNLL03\n",
    "# ======================================================\n",
    "MODEL_NAME = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"first\",  # cleaner span merging than \"simple\"\n",
    "    stride=64,                     # 64-token overlap between windows ‚Äî no missed entities\n",
    ")\n",
    "print(f\"Model loaded : {MODEL_NAME}\")\n",
    "print(f\"Labels       : {model.config.id2label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5da001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts loaded. FR words: 406 | EN words: 365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# CV text  (French or English ‚Äî model handles both)\n",
    "# ======================================================\n",
    "text_fr = \"\"\"\n",
    "Mohammed JADIR\n",
    "\n",
    "√âtudiant Ing√©nieur en Data Science & Intelligence Artificielle\n",
    "\n",
    "+212 6 82 84 65 80\n",
    "mohamad.jadir2018@gmail.com\n",
    "GitHub : github.com/Jadir99\n",
    "LinkedIn : mohammed-jadir\n",
    "Portfolio : jadir-mohammed.com\n",
    "\n",
    "FORMATION\n",
    "\n",
    "Diplome d'Ing√©nieur - Data Science, Big Data & Intelligence Artificielle\n",
    "√âcole Nationale Sup√©rieure de l'Intelligence Artificielle et Sciences des Donn√©es (ENSIASD), Taroudant\n",
    "2023 - Pr√©sent\n",
    "\n",
    "Diplome Universitaire de Technologie (DUT) - G√©nie Informatique\n",
    "√âcole Sup√©rieure de Technologie (EST), Universit√© Cadi Ayyad, Safi\n",
    "2021 - 2023\n",
    "\n",
    "EXP√âRIENCE PROFESSIONNELLE\n",
    "\n",
    "Stagiaire Data Science - Banque Centrale Populaire (Stage PFA)\n",
    "Juillet 2025 - Septembre 2025\n",
    "D√©veloppement d'un mod√®le hybride CamemBERT + Random Forest pour la classification automatique des r√©clamations clients.\n",
    "Conception de dashboards interactifs avec Power BI pour l'aide √† la d√©cision.\n",
    "Analyse et traitement des donn√©es avec Pandas et PostgreSQL.\n",
    "D√©veloppement et d√©ploiement d'API avec Flask.\n",
    "\n",
    "Stagiaire Data Science - YaneCode\n",
    "Juillet 2024 - Ao√ªt 2024\n",
    "D√©veloppement d'une plateforme e-learning intelligente int√©grant un syst√®me de recommandation bas√© sur l'IA.\n",
    "Utilisation de Laravel, Flask, TensorFlow, Gemini LLM, Scikit-learn, NumPy et Pandas.\n",
    "Conception de l'architecture backend et entra√Ænement des mod√®les de recommandation.\n",
    "Int√©gration des algorithmes IA dans l'application web.\n",
    "\n",
    "PROJETS\n",
    "\n",
    "SmartCourseQA - Chatbot Intelligent & G√©n√©rateur de Quiz\n",
    "github.com/Jadir99/SmartCourseQA\n",
    "D√©veloppement d'une application RAG hybride (FAISS, BM25, GPT-4o-mini, multilingual-e5-large).\n",
    "Chatbot acad√©mique, g√©n√©ration automatique de QCM et syst√®me d'√©valuation.\n",
    "Backend d√©velopp√© avec Flask.\n",
    "\n",
    "Syst√®me de Questions-R√©ponses sur Documents PDF\n",
    "github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
    "Impl√©mentation d'un syst√®me RAG bas√© sur SentenceTransformers, FAISS et GPT-4o-mini.\n",
    "Interface interactive avec affichage des r√©f√©rences de pages et score de confiance.\n",
    "\n",
    "AZUL-AI - Plateforme Touristique IA\n",
    "azulaimaroc.com\n",
    "Transformation de l'exp√©rience touristique au Maroc via l'IA g√©n√©rative.\n",
    "Int√©gration de Gemini LLM, backend en Flask et API consomm√©e en JavaScript.\n",
    "\n",
    "Application de Visualisation - League of Legends\n",
    "github.com/Jadir99/league-of-legend-prediction-win\n",
    "Analyse et visualisation des donn√©es joueurs avec SQL, Django, Pandas, NumPy et Chart.js.\n",
    "\n",
    "Amazon Product Sentiment Analysis\n",
    "github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
    "Scraping automatis√© via Selenium.\n",
    "Analyse de sentiments avec RoBERTa.\n",
    "Visualisations avec Matplotlib, Seaborn et WordCloud.\n",
    "\n",
    "COMP√âTENCES TECHNIQUES\n",
    "\n",
    "Machine Learning / IA\n",
    "Python, TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn, NLP, Power BI\n",
    "\n",
    "D√©veloppement & Outils\n",
    "Laravel, Flask, Django, Hadoop, MapReduce, Java, Git, GitHub, Docker, Linux, n8n\n",
    "\n",
    "Bases de donn√©es\n",
    "MySQL, SQLite, PostgreSQL, MongoDB\n",
    "\n",
    "CERTIFICATIONS\n",
    "\n",
    "365 Data Science - Deep Learning with TensorFlow 2\n",
    "Oracle Cloud Infrastructure 2025 Certified Generative AI Professional\n",
    "CCNAv7 - Introduction to Networks\n",
    "\n",
    "LANGUES\n",
    "\n",
    "Arabe : Langue maternelle\n",
    "Fran√ßais : Interm√©diaire\n",
    "Anglais : Interm√©diaire\n",
    "\n",
    "ACTIVIT√âS PARASCOLAIRES\n",
    "\n",
    "Membre principal - Google Developer Group (GDG), Campus Universiapolis (2024-2025)\n",
    "Pr√©sident - Club Nakama (Culture asiatique) (2023-2024)\n",
    "Pr√©sident - Chess Club (2022-2023)\n",
    "\"\"\"\n",
    "\n",
    "text_en = \"\"\"\n",
    "Mohammed JADIR\n",
    "Engineering Student in Data Science & Artificial Intelligence\n",
    "+212 6 82 84 65 80 | mohamad.jadir2018@gmail.com\n",
    "GitHub: github.com/Jadir99 | LinkedIn: mohammed-jadir | Portfolio: jadir-mohammed.com\n",
    "\n",
    "EDUCATION\n",
    "Engineering Degree in Data Science, Big Data & Artificial Intelligence\n",
    "National School of Artificial Intelligence and Data Science (ENSIASD), Taroudant\n",
    "2023 - Present\n",
    "\n",
    "University Diploma of Technology (DUT) in Computer Engineering\n",
    "Higher School of Technology (EST), Cadi Ayyad University, Safi\n",
    "2021 - 2023\n",
    "\n",
    "PROFESSIONAL EXPERIENCE\n",
    "Data Science Intern - Banque Centrale Populaire (PFA Internship)\n",
    "July 2025 - September 2025\n",
    "Developed a hybrid CamemBERT + Random Forest model for automatic customer complaint classification.\n",
    "Designed interactive dashboards using Power BI to support decision-making.\n",
    "Processed and analyzed data using Pandas and PostgreSQL.\n",
    "Built and deployed APIs using Flask.\n",
    "\n",
    "Data Science Intern - YaneCode\n",
    "July 2024 - August 2024\n",
    "Developed an intelligent e-learning platform integrating an AI-based recommendation system.\n",
    "Used Laravel, Flask, TensorFlow, Gemini LLM, Scikit-learn, NumPy, and Pandas.\n",
    "Designed backend architecture and trained recommendation models.\n",
    "Integrated AI algorithms into the web application.\n",
    "\n",
    "PROJECTS\n",
    "SmartCourseQA - Intelligent Chatbot & Quiz Generator\n",
    "github.com/Jadir99/SmartCourseQA\n",
    "Developed a hybrid RAG application (FAISS, BM25, GPT-4o-mini, multilingual-e5-large).\n",
    "Academic chatbot with automatic MCQ generation and evaluation system.\n",
    "\n",
    "RAG-Based Document Question Answering System\n",
    "github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
    "Implemented a RAG-based QA system using SentenceTransformers, FAISS, and GPT-4o-mini.\n",
    "\n",
    "AZUL-AI - AI-Powered Tourism Platform\n",
    "azulaimaroc.com\n",
    "Enhanced the tourism experience in Morocco using generative AI.\n",
    "Integrated Gemini LLM, Flask backend, and JavaScript API consumption.\n",
    "\n",
    "League of Legends Data Visualization Application\n",
    "github.com/Jadir99/league-of-legend-prediction-win\n",
    "Data analysis and visualization using SQL, Django, Pandas, NumPy, and Chart.js.\n",
    "\n",
    "Amazon Product Sentiment Analysis\n",
    "github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
    "Automated product review extraction using Selenium.\n",
    "Sentiment analysis using RoBERTa.\n",
    "Data visualization with Matplotlib, Seaborn, and WordCloud.\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "Machine Learning & AI\n",
    "Python, TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn, NLP, Power BI\n",
    "\n",
    "Development & Tools\n",
    "Laravel, Flask, Django, Hadoop, MapReduce, Java, Git, GitHub, Docker, Linux, n8n\n",
    "\n",
    "Databases\n",
    "MySQL, SQLite, PostgreSQL, MongoDB\n",
    "\n",
    "CERTIFICATIONS\n",
    "365 Data Science - Deep Learning with TensorFlow 2\n",
    "Oracle Cloud Infrastructure 2025 - Certified Generative AI Professional\n",
    "CCNAv7 - Introduction to Networks\n",
    "\n",
    "LANGUAGES\n",
    "Arabic: Native\n",
    "French: Intermediate\n",
    "English: Intermediate\n",
    "\n",
    "EXTRACURRICULAR ACTIVITIES\n",
    "Core Member - Google Developer Group (GDG), Universiapolis Campus (2024-2025)\n",
    "President - Nakama Club (Asian Culture Club) (2023-2024)\n",
    "President - Chess Club (2022-2023)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texts loaded. FR words:\", len(text_fr.split()), \"| EN words:\", len(text_en.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8083a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing & helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# Preprocessing: clean text for the transformer\n",
    "# - Remove emojis & special bullets\n",
    "# - Normalize dashes and whitespace\n",
    "# - Keep structure (newlines) so sentence boundaries are clear\n",
    "# ======================================================\n",
    "def preprocess(text):\n",
    "    EMOJI = re.compile(\n",
    "        \"[\" + u\"\\U0001F300-\\U0001FAFF\" + u\"\\U00002300-\\U000027BF\" + \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = EMOJI.sub(\" \", text)\n",
    "    text = re.sub(r\"[üîπüî∏‚óè‚ñ™‚ñ∏‚ñ∫‚Ä¢‚ó¶]\", \" \", text)\n",
    "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Chunked NER:  split into ~400-word chunks with 40-word\n",
    "# overlap so long CVs are processed without truncation.\n",
    "# ======================================================\n",
    "def extract_ner(text, pipeline_fn, chunk_words=300, overlap=40):\n",
    "    words = text.split()\n",
    "    chunks, i = [], 0\n",
    "    while i < len(words):\n",
    "        chunks.append(\" \".join(words[i : i + chunk_words]))\n",
    "        i += chunk_words - overlap\n",
    "\n",
    "    all_ents = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():\n",
    "            all_ents.extend(pipeline_fn(chunk))\n",
    "    return all_ents\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# CV-aware label mapping\n",
    "# XLM-R Wikineural labels: PER, ORG, LOC, MISC -> richer CV categories\n",
    "# ======================================================\n",
    "NER_TO_CV = {\n",
    "    \"PER\":  \"Person / Candidate\",\n",
    "    \"ORG\":  \"Company / Organization / University\",\n",
    "    \"LOC\":  \"Location / City / Country\",\n",
    "    \"MISC\": \"Miscellaneous\",\n",
    "}\n",
    "\n",
    "def map_to_cv(entities):\n",
    "    out = {v: [] for v in NER_TO_CV.values()}\n",
    "    for e in entities:\n",
    "        label = e[\"entity_group\"]\n",
    "        cv_label = NER_TO_CV.get(label, \"Miscellaneous\")\n",
    "        out[cv_label].append((e[\"word\"].strip(), round(e[\"score\"], 3)))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Merge: case-insensitive dedup, keep highest score\n",
    "# ======================================================\n",
    "def dedup(entities_by_label):\n",
    "    result = {}\n",
    "    for label, items in entities_by_label.items():\n",
    "        seen = {}\n",
    "        for word, score in items:\n",
    "            key = word.lower()\n",
    "            if key not in seen or score > seen[key][1]:\n",
    "                seen[key] = (word, score)\n",
    "        result[label] = sorted(seen.values(), key=lambda x: -x[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Preprocessing & helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6cdf810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based extractor (v2 ‚Äî with regex personal info) defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# Rule-based extractor  (supplements transformer NER)\n",
    "# ======================================================\n",
    "\n",
    "TECH_SKILLS = [\n",
    "    \"Python\",\"TensorFlow\",\"PyTorch\",\"Scikit-learn\",\"Pandas\",\"NumPy\",\"Matplotlib\",\n",
    "    \"Seaborn\",\"NLP\",\"Power BI\",\"Flask\",\"Django\",\"Laravel\",\"Git\",\"GitHub\",\"Docker\",\n",
    "    \"Linux\",\"MySQL\",\"SQLite\",\"PostgreSQL\",\"MongoDB\",\"Hadoop\",\"MapReduce\",\"Java\",\n",
    "    \"SQL\",\"FAISS\",\"BM25\",\"SentenceTransformers\",\"RoBERTa\",\"CamemBERT\",\"Selenium\",\n",
    "    \"JavaScript\",\"Chart.js\",\"n8n\",\"Gemini LLM\",\"GPT-4o-mini\",\"RAG\",\"REST API\",\n",
    "    \"Keras\",\"XGBoost\",\"Spark\",\"FastAPI\",\"Random Forest\",\"Machine Learning\",\n",
    "    \"Deep Learning\",\"Generative AI\",\"LLM\",\"Jupyter\",\"Power BI\",\"WordCloud\",\n",
    "    \"Matplotlib\",\"TF-IDF\",\"BERT\",\"Transformer\",\"Hugging Face\",\"OpenAI\",\n",
    "]\n",
    "\n",
    "SOFT_SKILLS = [\n",
    "    \"Leadership\",\"Teamwork\",\"Communication\",\"Problem-solving\",\"Adaptability\",\n",
    "    \"Creativity\",\"Critical thinking\",\"Time management\",\"Autonomy\",\"Rigour\",\n",
    "]\n",
    "\n",
    "CERTIFICATIONS = [\n",
    "    \"365 Data Science\",\"Deep Learning with TensorFlow 2\",\n",
    "    \"Oracle Cloud Infrastructure 2025 Certified Generative AI Professional\",\n",
    "    \"Oracle Cloud Infrastructure 2025\",\"CCNAv7\",\"Introduction to Networks\",\n",
    "]\n",
    "\n",
    "LANGUAGES_KNOWN = [\n",
    "    \"Arabic\",\"French\",\"English\",\"Spanish\",\"Amazigh\",\n",
    "    \"Arabe\",\"Fran√ßais\",\"Anglais\",\n",
    "    \"Native\",\"Intermediate\",\"Beginner\",\"Fluent\",\"Bilingue\",\"Interm√©diaire\",\n",
    "    \"Langue maternelle\",\n",
    "]\n",
    "\n",
    "EDUCATION_KW = [\n",
    "    \"Engineering Degree\",\"Master\",\"Bachelor\",\"DUT\",\n",
    "    \"Diplome d'Ing√©nieur\",\"Diplome Universitaire de Technologie\",\n",
    "    \"Baccalaureate\",\"ENSIASD\",\"EST\",\"Cadi Ayyad\",\n",
    "]\n",
    "\n",
    "EXTRACURRICULAR_KW = [\n",
    "    \"Google Developer Group\",\"GDG\",\"Nakama Club\",\"Chess Club\",\n",
    "    \"President\",\"Vice-President\",\"Pr√©sident\",\"Universiapolis\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Regex patterns for personal information\n",
    "# -------------------------------------------------------\n",
    "REGEX_PATTERNS = {\n",
    "    \"Phone\":    r'\\+?[\\d\\s\\-().]{9,}',\n",
    "    \"Email\":    r'[\\w.\\-]+@[\\w.\\-]+\\.\\w{2,}',\n",
    "    \"GitHub\":   r'github\\.com/[\\w\\-/]+',\n",
    "    \"LinkedIn\": r'linkedin\\.com/in/[\\w\\-]+',\n",
    "    \"Website\":  r'(?:https?://)?(?:www\\.)?[\\w\\-]+\\.(?:com|ma|io|dev|ai|org|net)/[\\w\\-/]*',\n",
    "}\n",
    "\n",
    "# Detect person name: first non-empty line that looks like \"FIRSTNAME LASTNAME\"\n",
    "NAME_RE = re.compile(r'^[A-Z√Å√Ä√Ç√â√à√ä√ã√é√è√î√ô√õ√ú√á√ë][a-z√°√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√º√ß√±]+(?:\\s+[A-Z√Å√Ä√Ç√â√à√ä√ã√é√è√î√ô√õ√ú√á√ë][A-Z√Å√Ä√Ç√â√à√ä√ã√é√è√î√ô√õ√ú√á√ëa-z√°√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√º√ß√±]+)+$')\n",
    "\n",
    "\n",
    "def rule_based_extract(text):\n",
    "    found = {\n",
    "        \"Person / Name\":         [],\n",
    "        \"Contact Information\":   [],\n",
    "        \"Technical Skills\":      [],\n",
    "        \"Soft Skills\":           [],\n",
    "        \"Certifications\":        [],\n",
    "        \"Languages\":             [],\n",
    "        \"Education Keywords\":    [],\n",
    "        \"Extracurricular\":       [],\n",
    "    }\n",
    "    t = text.lower()\n",
    "\n",
    "    # --- Person name (first line matching pattern) ---\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and NAME_RE.match(line):\n",
    "            found[\"Person / Name\"].append((line, 1.0))\n",
    "            break   # only grab the first match (resume owner)\n",
    "\n",
    "    # --- Contact info via regex ---\n",
    "    for field, pattern in REGEX_PATTERNS.items():\n",
    "        for m in re.finditer(pattern, text):\n",
    "            val = m.group().strip()\n",
    "            if len(val) > 4:\n",
    "                found[\"Contact Information\"].append((f\"{field}: {val}\", 1.0))\n",
    "\n",
    "    # --- Tech skills ---\n",
    "    for kw in TECH_SKILLS:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Technical Skills\"].append((kw, 1.0))\n",
    "\n",
    "    # --- Soft skills ---\n",
    "    for kw in SOFT_SKILLS:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Soft Skills\"].append((kw, 1.0))\n",
    "\n",
    "    # --- Certifications ---\n",
    "    for kw in CERTIFICATIONS:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Certifications\"].append((kw, 1.0))\n",
    "\n",
    "    # --- Languages ---\n",
    "    for kw in LANGUAGES_KNOWN:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Languages\"].append((kw, 1.0))\n",
    "\n",
    "    # --- Education keywords ---\n",
    "    for kw in EDUCATION_KW:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Education Keywords\"].append((kw, 1.0))\n",
    "\n",
    "    # --- Extracurricular ---\n",
    "    for kw in EXTRACURRICULAR_KW:\n",
    "        if kw.lower() in t:\n",
    "            found[\"Extracurricular\"].append((kw, 1.0))\n",
    "\n",
    "    # Dedup each category\n",
    "    for key in found:\n",
    "        seen = {}\n",
    "        for word, score in found[key]:\n",
    "            k = word.lower()\n",
    "            if k not in seen:\n",
    "                seen[k] = (word, score)\n",
    "        found[key] = list(seen.values())\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "print(\"Rule-based extractor (v2 ‚Äî with regex personal info) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a34c654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================\n",
      "  CV EXTRACTION RESULTS ‚Äî FR\n",
      "====================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [XLM-RoBERTa NER ‚Äî transformer entities]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "  [Company / Organization / University]  (12 entities)\n",
      "    ‚Ä¢ √âcole Nationale Sup√©rieure de l ' Intelligence Artificielle et Sciences des Donn√©es ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.914\n",
      "    ‚Ä¢ Big Data & Intelligence Artificielle       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.767\n",
      "    ‚Ä¢ Centrale Populaire                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.702\n",
      "    ‚Ä¢ Google Developer Group                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 0.700\n",
      "    ‚Ä¢ √âcole Sup√©rieure de Technologie            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 0.698\n",
      "    ‚Ä¢ Science                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 0.617\n",
      "    ‚Ä¢ Club                                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.595\n",
      "    ‚Ä¢ Informatique                               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.517\n",
      "    ‚Ä¢ Universit√© Cadi Ayyad                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.502\n",
      "    ‚Ä¢ de Technologie                             ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.430\n",
      "    ‚Ä¢ Ing√©nieur                                  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.427\n",
      "    ‚Ä¢ Artificielle                               ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.400\n",
      "\n",
      "  [Location / City / Country]  (3 entities)\n",
      "    ‚Ä¢ Taroudant                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.986\n",
      "    ‚Ä¢ Maroc                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.867\n",
      "    ‚Ä¢ Campus Universiapolis                      ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.476\n",
      "\n",
      "  [Miscellaneous]  (75 entities)\n",
      "    ‚Ä¢ League of Legends                          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.973\n",
      "    ‚Ä¢ TensorFlow                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.952\n",
      "    ‚Ä¢ Java                                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.920\n",
      "    ‚Ä¢ Pandas                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.916\n",
      "    ‚Ä¢ Flask                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.914\n",
      "    ‚Ä¢ NumPy                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.914\n",
      "    ‚Ä¢ Hadoop                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.909\n",
      "    ‚Ä¢ GitHub                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.909\n",
      "    ‚Ä¢ Docker                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.909\n",
      "    ‚Ä¢ Matplotlib                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.907\n",
      "    ‚Ä¢ Git                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.907\n",
      "    ‚Ä¢ Django                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.906\n",
      "    ‚Ä¢ MySQL                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.902\n",
      "    ‚Ä¢ Linux                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.901\n",
      "    ‚Ä¢ MapReduce                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.895\n",
      "    ‚Ä¢ PyTorch                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.894\n",
      "    ‚Ä¢ Amazon Product Sentiment Analysis github   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.884\n",
      "    ‚Ä¢ Deep Learning                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.884\n",
      "    ‚Ä¢ Seaborn                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.882\n",
      "    ‚Ä¢ WordCloud                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.878\n",
      "    ... (+55 more)\n",
      "\n",
      "  NER entities total: 90\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [Rule-based extraction ‚Äî keywords + regex]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "  [Person / Name]  (1 entities)\n",
      "    ‚Ä¢ Mohammed JADIR\n",
      "\n",
      "  [Contact Information]  (16 entities)\n",
      "    ‚Ä¢ Phone: +212 6 82 84 65 80\n",
      "    ‚Ä¢ Phone: 2021 - 2023\n",
      "    ‚Ä¢ Phone: (2024-2025)\n",
      "    ‚Ä¢ Phone: ) (2023-2024)\n",
      "    ‚Ä¢ Phone: (2022-2023)\n",
      "    ‚Ä¢ Email: mohamad.jadir2018@gmail.com\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/SmartCourseQA\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/league-of-legend-prediction-win\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
      "    ‚Ä¢ Website: github.com/Jadir99\n",
      "    ‚Ä¢ Website: github.com/Jadir99/SmartCourseQA\n",
      "    ‚Ä¢ Website: github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
      "    ‚Ä¢ Website: github.com/Jadir99/league-of-legend-prediction-win\n",
      "    ‚Ä¢ Website: github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
      "\n",
      "  [Technical Skills]  (45 entities)\n",
      "    ‚Ä¢ Python\n",
      "    ‚Ä¢ TensorFlow\n",
      "    ‚Ä¢ PyTorch\n",
      "    ‚Ä¢ Scikit-learn\n",
      "    ‚Ä¢ Pandas\n",
      "    ‚Ä¢ NumPy\n",
      "    ‚Ä¢ Matplotlib\n",
      "    ‚Ä¢ Seaborn\n",
      "    ‚Ä¢ NLP\n",
      "    ‚Ä¢ Power BI\n",
      "    ‚Ä¢ Flask\n",
      "    ‚Ä¢ Django\n",
      "    ‚Ä¢ Laravel\n",
      "    ‚Ä¢ Git\n",
      "    ‚Ä¢ GitHub\n",
      "    ‚Ä¢ Docker\n",
      "    ‚Ä¢ Linux\n",
      "    ‚Ä¢ MySQL\n",
      "    ‚Ä¢ SQLite\n",
      "    ‚Ä¢ PostgreSQL\n",
      "    ‚Ä¢ MongoDB\n",
      "    ‚Ä¢ Hadoop\n",
      "    ‚Ä¢ MapReduce\n",
      "    ‚Ä¢ Java\n",
      "    ‚Ä¢ SQL\n",
      "    ‚Ä¢ FAISS\n",
      "    ‚Ä¢ BM25\n",
      "    ‚Ä¢ SentenceTransformers\n",
      "    ‚Ä¢ RoBERTa\n",
      "    ‚Ä¢ CamemBERT\n",
      "    ‚Ä¢ Selenium\n",
      "    ‚Ä¢ JavaScript\n",
      "    ‚Ä¢ Chart.js\n",
      "    ‚Ä¢ n8n\n",
      "    ‚Ä¢ Gemini LLM\n",
      "    ‚Ä¢ GPT-4o-mini\n",
      "    ‚Ä¢ RAG\n",
      "    ‚Ä¢ Random Forest\n",
      "    ‚Ä¢ Machine Learning\n",
      "    ‚Ä¢ Deep Learning\n",
      "    ‚Ä¢ Generative AI\n",
      "    ‚Ä¢ LLM\n",
      "    ‚Ä¢ WordCloud\n",
      "    ‚Ä¢ BERT\n",
      "    ‚Ä¢ Transformer\n",
      "\n",
      "  [Certifications]  (6 entities)\n",
      "    ‚Ä¢ 365 Data Science\n",
      "    ‚Ä¢ Deep Learning with TensorFlow 2\n",
      "    ‚Ä¢ Oracle Cloud Infrastructure 2025 Certified Generative AI Professional\n",
      "    ‚Ä¢ Oracle Cloud Infrastructure 2025\n",
      "    ‚Ä¢ CCNAv7\n",
      "    ‚Ä¢ Introduction to Networks\n",
      "\n",
      "  [Languages]  (5 entities)\n",
      "    ‚Ä¢ Arabe\n",
      "    ‚Ä¢ Fran√ßais\n",
      "    ‚Ä¢ Anglais\n",
      "    ‚Ä¢ Interm√©diaire\n",
      "    ‚Ä¢ Langue maternelle\n",
      "\n",
      "  [Education Keywords]  (6 entities)\n",
      "    ‚Ä¢ DUT\n",
      "    ‚Ä¢ Diplome d'Ing√©nieur\n",
      "    ‚Ä¢ Diplome Universitaire de Technologie\n",
      "    ‚Ä¢ ENSIASD\n",
      "    ‚Ä¢ EST\n",
      "    ‚Ä¢ Cadi Ayyad\n",
      "\n",
      "  [Extracurricular]  (5 entities)\n",
      "    ‚Ä¢ Google Developer Group\n",
      "    ‚Ä¢ GDG\n",
      "    ‚Ä¢ Chess Club\n",
      "    ‚Ä¢ Pr√©sident\n",
      "    ‚Ä¢ Universiapolis\n",
      "\n",
      "  Rule-based entities total: 84\n",
      "\n",
      "====================================================================\n",
      "  Grand total (FR): 174 entities detected\n",
      "====================================================================\n",
      "\n",
      "\n",
      "====================================================================\n",
      "  CV EXTRACTION RESULTS ‚Äî EN\n",
      "====================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [XLM-RoBERTa NER ‚Äî transformer entities]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "  [Company / Organization / University]  (17 entities)\n",
      "    ‚Ä¢ National School of Artificial Intelligence and Data Science ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.940\n",
      "    ‚Ä¢ Cadi Ayyad University                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.900\n",
      "    ‚Ä¢ Nakama Club                                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.886\n",
      "    ‚Ä¢ Asian Culture Club                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.857\n",
      "    ‚Ä¢ Google Developer Group                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.854\n",
      "    ‚Ä¢ Computer Engineering Higher School of Technology ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.831\n",
      "    ‚Ä¢ Chess Club                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.829\n",
      "    ‚Ä¢ Intelligence                               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.716\n",
      "    ‚Ä¢ Campus                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.582\n",
      "    ‚Ä¢ Data &                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.535\n",
      "    ‚Ä¢ LLM                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.532\n",
      "    ‚Ä¢ ENSIASD                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.528\n",
      "    ‚Ä¢ GDG                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.525\n",
      "    ‚Ä¢ Science                                    ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.481\n",
      "    ‚Ä¢ - learn                                    ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.455\n",
      "    ‚Ä¢ Technology                                 ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.449\n",
      "    ‚Ä¢ EST                                        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.436\n",
      "\n",
      "  [Location / City / Country]  (4 entities)\n",
      "    ‚Ä¢ Taroudant                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.984\n",
      "    ‚Ä¢ Morocco                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.919\n",
      "    ‚Ä¢ Safi                                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 0.684\n",
      "    ‚Ä¢ Universiapolis                             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 0.501\n",
      "\n",
      "  [Miscellaneous]  (72 entities)\n",
      "    ‚Ä¢ League of Legends Data Visualization Application ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.975\n",
      "    ‚Ä¢ TensorFlow                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.971\n",
      "    ‚Ä¢ Pandas                                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.962\n",
      "    ‚Ä¢ PostgreSQL                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.961\n",
      "    ‚Ä¢ JavaScript                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.959\n",
      "    ‚Ä¢ SentenceTransformers                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.951\n",
      "    ‚Ä¢ SQL                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.951\n",
      "    ‚Ä¢ Flask                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.949\n",
      "    ‚Ä¢ Matplotlib                                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.948\n",
      "    ‚Ä¢ Amazon Product Sentiment Analysis          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.947\n",
      "    ‚Ä¢ Chart                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.946\n",
      "    ‚Ä¢ Selenium                                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.946\n",
      "    ‚Ä¢ RoBERTa                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.946\n",
      "    ‚Ä¢ SmartCourseQA                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.945\n",
      "    ‚Ä¢ Native French                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.940\n",
      "    ‚Ä¢ Seaborn                                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.939\n",
      "    ‚Ä¢ WordCloud                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.938\n",
      "    ‚Ä¢ Power BI Development & Tools Laravel       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.937\n",
      "    ‚Ä¢ AI Python                                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.933\n",
      "    ‚Ä¢ Deep Learning                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.933\n",
      "    ... (+52 more)\n",
      "\n",
      "  NER entities total: 93\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [Rule-based extraction ‚Äî keywords + regex]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "  [Person / Name]  (1 entities)\n",
      "    ‚Ä¢ Mohammed JADIR\n",
      "\n",
      "  [Contact Information]  (16 entities)\n",
      "    ‚Ä¢ Phone: +212 6 82 84 65 80\n",
      "    ‚Ä¢ Phone: 2021 - 2023\n",
      "    ‚Ä¢ Phone: (2024-2025)\n",
      "    ‚Ä¢ Phone: ) (2023-2024)\n",
      "    ‚Ä¢ Phone: (2022-2023)\n",
      "    ‚Ä¢ Email: mohamad.jadir2018@gmail.com\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/SmartCourseQA\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/league-of-legend-prediction-win\n",
      "    ‚Ä¢ GitHub: github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
      "    ‚Ä¢ Website: github.com/Jadir99\n",
      "    ‚Ä¢ Website: github.com/Jadir99/SmartCourseQA\n",
      "    ‚Ä¢ Website: github.com/Jadir99/RAG-Based-Document-Question-Answering-System\n",
      "    ‚Ä¢ Website: github.com/Jadir99/league-of-legend-prediction-win\n",
      "    ‚Ä¢ Website: github.com/Jadir99/Sentiment-analysis-from-product-reviews\n",
      "\n",
      "  [Technical Skills]  (45 entities)\n",
      "    ‚Ä¢ Python\n",
      "    ‚Ä¢ TensorFlow\n",
      "    ‚Ä¢ PyTorch\n",
      "    ‚Ä¢ Scikit-learn\n",
      "    ‚Ä¢ Pandas\n",
      "    ‚Ä¢ NumPy\n",
      "    ‚Ä¢ Matplotlib\n",
      "    ‚Ä¢ Seaborn\n",
      "    ‚Ä¢ NLP\n",
      "    ‚Ä¢ Power BI\n",
      "    ‚Ä¢ Flask\n",
      "    ‚Ä¢ Django\n",
      "    ‚Ä¢ Laravel\n",
      "    ‚Ä¢ Git\n",
      "    ‚Ä¢ GitHub\n",
      "    ‚Ä¢ Docker\n",
      "    ‚Ä¢ Linux\n",
      "    ‚Ä¢ MySQL\n",
      "    ‚Ä¢ SQLite\n",
      "    ‚Ä¢ PostgreSQL\n",
      "    ‚Ä¢ MongoDB\n",
      "    ‚Ä¢ Hadoop\n",
      "    ‚Ä¢ MapReduce\n",
      "    ‚Ä¢ Java\n",
      "    ‚Ä¢ SQL\n",
      "    ‚Ä¢ FAISS\n",
      "    ‚Ä¢ BM25\n",
      "    ‚Ä¢ SentenceTransformers\n",
      "    ‚Ä¢ RoBERTa\n",
      "    ‚Ä¢ CamemBERT\n",
      "    ‚Ä¢ Selenium\n",
      "    ‚Ä¢ JavaScript\n",
      "    ‚Ä¢ Chart.js\n",
      "    ‚Ä¢ n8n\n",
      "    ‚Ä¢ Gemini LLM\n",
      "    ‚Ä¢ GPT-4o-mini\n",
      "    ‚Ä¢ RAG\n",
      "    ‚Ä¢ Random Forest\n",
      "    ‚Ä¢ Machine Learning\n",
      "    ‚Ä¢ Deep Learning\n",
      "    ‚Ä¢ Generative AI\n",
      "    ‚Ä¢ LLM\n",
      "    ‚Ä¢ WordCloud\n",
      "    ‚Ä¢ BERT\n",
      "    ‚Ä¢ Transformer\n",
      "\n",
      "  [Certifications]  (5 entities)\n",
      "    ‚Ä¢ 365 Data Science\n",
      "    ‚Ä¢ Deep Learning with TensorFlow 2\n",
      "    ‚Ä¢ Oracle Cloud Infrastructure 2025\n",
      "    ‚Ä¢ CCNAv7\n",
      "    ‚Ä¢ Introduction to Networks\n",
      "\n",
      "  [Languages]  (5 entities)\n",
      "    ‚Ä¢ Arabic\n",
      "    ‚Ä¢ French\n",
      "    ‚Ä¢ English\n",
      "    ‚Ä¢ Native\n",
      "    ‚Ä¢ Intermediate\n",
      "\n",
      "  [Education Keywords]  (5 entities)\n",
      "    ‚Ä¢ Engineering Degree\n",
      "    ‚Ä¢ DUT\n",
      "    ‚Ä¢ ENSIASD\n",
      "    ‚Ä¢ EST\n",
      "    ‚Ä¢ Cadi Ayyad\n",
      "\n",
      "  [Extracurricular]  (6 entities)\n",
      "    ‚Ä¢ Google Developer Group\n",
      "    ‚Ä¢ GDG\n",
      "    ‚Ä¢ Nakama Club\n",
      "    ‚Ä¢ Chess Club\n",
      "    ‚Ä¢ President\n",
      "    ‚Ä¢ Universiapolis\n",
      "\n",
      "  Rule-based entities total: 83\n",
      "\n",
      "====================================================================\n",
      "  Grand total (EN): 176 entities detected\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# Display helper\n",
    "# ======================================================\n",
    "def display_cv_results(ner_results, rule_results, lang=\"FR\"):\n",
    "    W = 68\n",
    "    print(f\"\\n{'='*W}\")\n",
    "    print(f\"  CV EXTRACTION RESULTS ‚Äî {lang}\")\n",
    "    print(f\"{'='*W}\")\n",
    "\n",
    "    # --- Transformer NER ---\n",
    "    print(f\"\\n{'‚îÄ'*W}\")\n",
    "    print(\"  [XLM-RoBERTa NER ‚Äî transformer entities]\")\n",
    "    print(f\"{'‚îÄ'*W}\")\n",
    "    total_ner = 0\n",
    "    for category, items in ner_results.items():\n",
    "        if items:\n",
    "            print(f\"\\n  [{category}]  ({len(items)} entities)\")\n",
    "            for word, score in items[:20]:   # cap at 20 to keep output readable\n",
    "                bar = \"‚ñà\" * int(score * 10) + \"‚ñë\" * (10 - int(score * 10))\n",
    "                print(f\"    ‚Ä¢ {word:<42} {bar} {score:.3f}\")\n",
    "            if len(items) > 20:\n",
    "                print(f\"    ... (+{len(items)-20} more)\")\n",
    "            total_ner += len(items)\n",
    "    print(f\"\\n  NER entities total: {total_ner}\")\n",
    "\n",
    "    # --- Rule-based ---\n",
    "    print(f\"\\n{'‚îÄ'*W}\")\n",
    "    print(\"  [Rule-based extraction ‚Äî keywords + regex]\")\n",
    "    print(f\"{'‚îÄ'*W}\")\n",
    "    total_rule = 0\n",
    "    for category, items in rule_results.items():\n",
    "        if items:\n",
    "            print(f\"\\n  [{category}]  ({len(items)} entities)\")\n",
    "            for word, _ in items:\n",
    "                print(f\"    ‚Ä¢ {word}\")\n",
    "            total_rule += len(items)\n",
    "    print(f\"\\n  Rule-based entities total: {total_rule}\")\n",
    "\n",
    "    print(f\"\\n{'='*W}\")\n",
    "    print(f\"  Grand total ({lang}): {total_ner + total_rule} entities detected\")\n",
    "    print(f\"{'='*W}\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Run full pipeline\n",
    "# ======================================================\n",
    "for lang, raw_text in [(\"FR\", text_fr), (\"EN\", text_en)]:\n",
    "    clean     = preprocess(raw_text)\n",
    "    ner_cvmap = dedup(map_to_cv(extract_ner(clean, nlp)))\n",
    "    rule_ents = rule_based_extract(clean)\n",
    "    display_cv_results(ner_cvmap, rule_ents, lang=lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "528a9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang                                            EN  FR\n",
      "Source     Category                                   \n",
      "Rule-based Certifications                        5   6\n",
      "           Contact Information                  16  16\n",
      "           Education Keywords                    5   6\n",
      "           Extracurricular                       6   5\n",
      "           Languages                             5   5\n",
      "           Person / Name                         1   1\n",
      "           Soft Skills                           0   0\n",
      "           Technical Skills                     45  45\n",
      "XLM-R NER  Company / Organization / University  17  12\n",
      "           Location / City / Country             4   3\n",
      "           Miscellaneous                        72  75\n",
      "           Person / Candidate                    0   0\n",
      "\n",
      "XLM-R NER   total : 183\n",
      "Rule-based  total : 167\n",
      "Grand total        : 350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# Summary table: entity counts per source + category\n",
    "# ======================================================\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for lang, raw_text in [(\"FR\", text_fr), (\"EN\", text_en)]:\n",
    "    clean   = preprocess(raw_text)\n",
    "    ner_map = dedup(map_to_cv(extract_ner(clean, nlp)))\n",
    "    rule    = rule_based_extract(clean)\n",
    "\n",
    "    for cat, items in ner_map.items():\n",
    "        rows.append({\"Lang\": lang, \"Source\": \"XLM-R NER\",  \"Category\": cat, \"Count\": len(items)})\n",
    "    for cat, items in rule.items():\n",
    "        rows.append({\"Lang\": lang, \"Source\": \"Rule-based\", \"Category\": cat, \"Count\": len(items)})\n",
    "\n",
    "df    = pd.DataFrame(rows)\n",
    "pivot = (\n",
    "    df.pivot_table(index=[\"Source\", \"Category\"], columns=\"Lang\", values=\"Count\", aggfunc=\"sum\")\n",
    "      .fillna(0).astype(int)\n",
    ")\n",
    "print(pivot.to_string())\n",
    "ner_total  = df[df.Source == \"XLM-R NER\"][\"Count\"].sum()\n",
    "rule_total = df[df.Source == \"Rule-based\"][\"Count\"].sum()\n",
    "print(f\"\\nXLM-R NER   total : {ner_total}\")\n",
    "print(f\"Rule-based  total : {rule_total}\")\n",
    "print(f\"Grand total        : {ner_total + rule_total}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
